# `Scrapy`常用工具命令

`Scrapy`中工具命令分为全局命令和项目命令，全局命令是不需要依靠`Scrapy`就可以在全局中直接运行的命令，而项目命令则需要在`Scrapy`项目下才能运行。

## 1 全局命令

在不进入`Scrapy`项目目录的情况下运行`scrapy -h`查看所有全局命令：

```
(py3env) tys@tys:~$ scrapy -h
Scrapy 1.5.1 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy <command> -h" to see more info about a command
```

其中`bench`比较特殊，虽然在这里显示，但仍归为项目命令。

### 1.1 `fetch`

用来显示爬虫爬取的过程，通过`scrapy fetch url`的形式显示爬取对应网站的过程。此时如果在`Scrapy`项目目录外使用该命令，则会调用`Scrapy`默认的爬虫来进行网页的爬取，如果在`Scrapy`的某个项目内使用，就会调用该项目内的爬虫来进行网页的爬取。

在使用`fetch`命令的时候，可以使用某些参数进行相应的控制。使用`scrapy fetch -h`列出所有可以使用的参数：

```
(py3env) tys@tys:~$ scrapy fetch -h
Usage
=====
  scrapy fetch [options] <url>

Fetch a URL using the Scrapy downloader and print its content to stdout. You
may want to use --nolog to disable logging

Options
=======
--help, -h              show this help message and exit
--spider=SPIDER         use this spider
--headers               print response HTTP headers instead of body
--no-redirect           do not handle HTTP 3xx status codes and print response
                        as-is

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure
```

### 1.2 `runspider`

该命令可以不依托`Scrapy`的爬虫项目直接运行一个爬虫文件。

### 1.3 `settings`

查看对应的配置信息。如果在项目目录内使用`settings`命令，查看的是对应项目的配置信息，实际上就是项目路径下的`settings.py`文件中的配置信息，在目录外使用查看默认配置信息。

使用`-h`参数查看详细使用：

```
(py3env) tys@tys:~$ scrapy settings -h
Usage
=====
  scrapy settings [options]

Get settings values

Options
=======
--help, -h              show this help message and exit
--get=SETTING           print raw setting value
--getbool=SETTING       print setting value, interpreted as a boolean
--getint=SETTING        print setting value, interpreted as an integer
--getfloat=SETTING      print setting value, interpreted as a float
--getlist=SETTING       print setting value, interpreted as a list

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure

```

使用`--get`参数，获取对应配置信息的值：

```
(py3env) tys@tys:~$ scrapy settings --get BOT_NAME
scrapybot
```

### 1.4 `shell`

启动`Scrapy`交互终端`(Scrapy shell)`，交互终端可以实现在不启动`Scrapy`爬虫的情况下，对网站响应进行调试。比如爬取百度网页，注意如果网址带有参数则需要加引号包围：

```
(py3env) tys@tys:~$ scrapy shell http://www.baidu.com --nolog
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7efef957bc18>
[s]   item       {}
[s]   request    <GET http://www.baidu.com>
[s]   response   <200 http://www.baidu.com>
[s]   settings   <scrapy.settings.Settings object at 0x7efefe2e2cf8>
[s]   spider     <DefaultSpider 'default' at 0x7efef74ad710>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects 
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>> 
```

可以看见执行该命令后会出现响应的对象和快捷命令，在下方输入交互命令进行相应的调试。

调试完后执行`exit()`命令退出交互模式。

同样可以使用`-h`参数查看具体用法和参数。

#### 1.5 `startproject`

之前已经使用过，主要用于创建项目，后面接项目名称：

```
scrapy startproject project_name
```

可以使用`-h`查看使用详情：

```
(py3env) tys@tys:~/project$ scrapy startproject -h
Usage
=====
  scrapy startproject <project_name> [project_dir]

Create new project

Options
=======
--help, -h              show this help message and exit

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure

```

`--logfile=FILE`:指定日志文件，`FILE`为指定的日志文件的路径地址。

`--loglevel=LEVEL`:控制日志信息的等级，默认为`DEBUG`模式，即会将对应的调试信息都输出，还可以设置为其它等级。

| 等级名   | 含义                             |
| -------- | -------------------------------- |
| CRITICAL | 发生最严重的错误                 |
| ERROR    | 发生必须立即处理的错误           |
| WARNIGN  | 出现一些警告信息，即存在潜在错误 |
| INFO     | 输出一些提示信息                 |
| DEBUG    | 输出一些调试信息，常用与开发阶段 |

`--nolog`:不输入日志信息。

### 1.6 `version`

查看`Scrapy`版本相关信息，在后面加上`-v`参数还可以查看与`Scrapy`相关的其他版本信息。

```
(py3env) tys@tys:~$ scrapy version
Scrapy 1.5.1
(py3env) tys@tys:~$ scrapy version -v
Scrapy       : 1.5.1
lxml         : 4.3.0.0
libxml2      : 2.9.9
cssselect    : 1.0.3
parsel       : 1.5.1
w3lib        : 1.19.0
Twisted      : 18.9.0
Python       : 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609]
pyOpenSSL    : 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018)
cryptography : 2.3.1
Platform     : Linux-4.4.0-138-generic-x86_64-with-debian-stretch-sid
```

### 1.7 `view`

可以下载某个网页并用浏览器查看。执行：

```
scrapy view http://news.163.com
```

会自动打开浏览器并展示已经下到本地的页面（网址是本地的网页地址）。

## 2 项目命令

必须在`scrapy`项目目录下使用，所以首先进入项目路径。

同样，在项目路径下使用`scrapy -h`查看可用命令：

```
(py3env) tys@tys:~/project/news163$ scrapy -h
Scrapy 1.5.1 - project: news163

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy <command> -h" to see more info about a command
```

在展示出来的命令中除了全局命令，剩下的就是 项目命令。

### 2.1 bench

测试本地硬件性能，会创建一个本地服务器并会以最大的速度爬行。

### 2.2 genspider

创建爬虫文件，可以基于现有的爬虫模板生成一个新的爬虫文件。

可以使用`-l`参数来查看当前可以使用的爬虫模板：

```
(py3env) tys@tys:~/project/news163$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
```

使用`-t`参数基于爬虫模板创建新的爬虫文件，如果使用的`basic`模版可以省略`-t`参数和模板名：

```
scrapy genspider -t spider_template spider_name url 
```

使用`-d`参数查看某个爬虫模板的内容：

```
(py3env) tys@tys:~/project/news163$ scrapy genspider -d basic
# -*- coding: utf-8 -*-
import scrapy


class $classname(scrapy.Spider):
    name = '$name'
    allowed_domains = ['$domain']
    start_urls = ['http://$domain/']

    def parse(self, response):
        pass
```

### 2.3 check

爬虫的测试比较麻烦，在`Scrapy`中使用合同(contract)的方式对爬虫进行测试，使用`check`命令可以实现对某个爬虫文件进行合同(contract)检查。

```
scrapy check spider_name
```

### 2.4 crawl

用来启动某个爬虫：

```
scrapy crawl spider_name
```

使用`-h`参数来查看使用详情和参数：

```
(py3env) tys@tys:~/project/news163$ scrapy crawl -h
Usage
=====
  scrapy crawl [options] <spider>

Run a spider

Options
=======
--help, -h              show this help message and exit
-a NAME=VALUE           set spider argument (may be repeated)
--output=FILE, -o FILE  dump scraped items into FILE (use - for stdout)
--output-format=FORMAT, -t FORMAT
                        format to use for dumping items with -o

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure
```

### 2.5 list

查看该项目下可以使用的所有爬虫文件名，即爬虫文件的`name`属性

```s
scrapy list
```

### 2.6 edit

直接打开对应编辑器对爬虫文件进行编辑，该命令在`linux`中使用比较方便，但在`windows`中执行可能会出现问题。

### 2.7 parse

获取指定的`URL`网址，并使用对应的爬虫文件进行处理和分析。

```
scrapy parse url
```

这里没有指定爬虫文件，也没有指定处理函数，就会使用默认的爬虫问价和处理函数进行相应的处理。

我们可以通过`scrapy parse -h`来查看该命令的参数：

```
(py3env) tys@tys:~/project/news163$ scrapy parse -h
Usage
=====
  scrapy parse [options] <url>

Parse URL (using its spider) and print the results

Options
=======
--help, -h              show this help message and exit
--spider=SPIDER         use this spider without looking for one
-a NAME=VALUE           set spider argument (may be repeated)
--pipelines             process items through pipelines
--nolinks               don't show links to follow (extracted requests)
--noitems               don't show scraped items
--nocolour              avoid using pygments to colorize the output
--rules, -r             use CrawlSpider rules to discover the callback
--callback=CALLBACK, -c CALLBACK
                        use this callback for parsing, instead looking for a
                        callback
--meta=META, -m META    inject extra meta into the Request, it must be a valid
                        raw json string
--depth=DEPTH, -d DEPTH
                        maximum depth for parsing requests [default: 1]
--verbose, -v           print each depth level one by one

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure
```

可以看到这些参数大致分为普通参数和全局参数，全局参数与其它命令的参数基本一致，该命令自身的参数有：

| 参数                             | 含义                               |
| -------------------------------- | ---------------------------------- |
| --spider=SPIDER                  | 强行指定某个爬虫文件spider进行处理 |
| -a NAME=VALUE                    | 设置spider的参数，可能会重复       |
| --pipelines                      | 通过pipelines来处理items           |
| --nolinks                        | 不展示提取到的链接信息             |
| --noitems                        | 不展示得到的items                  |
| --nocolour                       | 输入结果颜色不高亮                 |
| --rules，-r                      | 使用CrawlSpider规则去处理回调函数  |
| --callback=CALLBACK, -c CALLBACK | 指定spider中用于处理响应的回调函数 |
| --depth=DEPTH, -d DEPTH          | 设置爬行深度，默认深度为1          |
| --verbose                        | 显示每层的详细信息                 |
|                                  |                                    |

