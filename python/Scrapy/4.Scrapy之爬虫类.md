# Scrapy爬虫类

定义好Items后，就需要编写Spider文件用来爬取数据。

在`Scrapy`中，`Spider`负责的是网页的抓取逻辑，以及数据的解析逻辑。所以`Spider`是`Scrapy`爬虫中相对核心的部分。

## 1 `Spider`类

这个类是`Spider`最基础的类，里面只定义两三个默认的方法，非常的简单。其他的种类的`Spider`都是继承自这个`Spider`的。而我们的`Spider`也都需要继承这个类。

### 1.1 创建爬虫

在之前的命令中，我们提到，一般来说我们需要使用`Scrapy`的命令行生成一个`Spider`模板。命令的语法是这样的：

```
scrapy genspider [options] <name> <domain>
```

这里有两个必写的参数，`name`指的是这个`Spider`的名称，而且这个名称必须是唯一的，不能有其他的`Spider`重名。而`domain`指的是这个`Spider`爬取的域名。

除此之外，这个命令还有以下可选的options：

```
Options
=======
--help, -h              show this help message and exit
--list, -l              List available templates
--edit, -e              Edit spider after creating it
--dump=TEMPLATE, -d TEMPLATE
                        Dump template to standard output
--template=TEMPLATE, -t TEMPLATE
                        Uses a custom template.
--force                 If the spider already exists, overwrite it with the
                        template
```

该命令会在`spider`文件夹下生成一个`spidername.py`文件，`spidername`就是爬虫名。

在创建爬虫文件之前可以先查看有哪些模板：

```
(py3env) tys@tys:~/project/news163$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

```

可以看见有四种模板，可以使用`-t`参数指定模板，默认使用basic模板（这里讨论），其它模板在下面会有详细介绍。

如果使用的是基础(basic)模板（默认），将会创建出以下格式的模板：

```
# -*- coding: utf-8 -*-
import scrapy # 导入模块


class SpidernameSpider(scrapy.Spider):  # 默认名称，可修改，继承自scrapy.Spider基类
    name = 'spidername' # 创建时爬虫文件时传入的爬虫名，可修改
    allowed_domains = ['mydomain.com'] # 允许爬取的域名，默认在创建文件时传入，可修改
    start_urls = ['http://mydomain.com/'] # 爬行的起始网址，可定义多个，逗号隔开

    def parse(self, response):  # 如果没有特别指定回调函数，该方法就是处理爬到的网页响应的默认方法
        pass
```

可以看到，这个模版的主体为一个`SpidernameSpider`类，继承自`scrapy.Spider`，这个类的名称与文件名基本保持一致。

### 1.2 Spider类的属性和方法

通过查看源码发现，Spider基类中除了上面显示的属性和方法外，还定义了其它的属性和方法，它们是：

- **name**

  一个定义了`Spider`名称的字符串，`Scrapy`会通过这个属性来定位`Spider`，所以这个`name`必须是唯一的。

  如果`Spider`抓取的是单个域名的内容，那么通常的做法是使用域名来对`Spider`命名。例如，一个抓取`mywebsite.com`的`Spider`应该命名为`mywebsite`。

- **allowed_domains**

  一个可选的列表，定义了允许访问的域名。不包含在此域名列表中的URL将不会被跟进爬取。

- **start_urls**

  一个爬虫开始爬取的起始URL列表，也是一个可选项。爬虫启动后，将首先从这些URL开始爬取。

- **custom_settings**

  配置`Spider`自定义设置的字典，会在运行这个`Spider`时覆盖项目级别的设置。这个属性需要被设置为一个类属性，因为它需要在实例化之前被定义好。

- **crawler**

  这个属性将会在类实例化之后，由`from_crawler()`设置，并且连接到绑定的`Crawler`对象。`Crawler`对象在项目中封装了许多组件，供单独对应访问。

- **settings**

  运行此`Spider`的配置，是一个`Settings`对象。

- **logger**

  这是一个由`Spider`的名称创建的`Python logger`对象，可以使用它来输出信息。

- **start_requests()**

  这个方法会返回一个可迭代对象，里面包含着`Spider`启动时首先抓取的`Request`。`Scrapy`会在这个爬虫启动时默认调用此方法一次，默认的实现是将`start_urls`中的每一个url生成一个`Request`。

  如果你希望改变`Spider`抓取起始url的逻辑，那么你需要重写此方法。

  如使用post请求：

  ```
  def start_requests(self):
  	for url in start_urls:
  		form_data = {}
  		yield scrapy.FormRequest(url=url, formdata=form_data), callback=None)
  ```

- **parse(response)**

  当`Request`没有指定回调函数时，会`Scrapy`默认的调用这个方法来处理response。这个方法的通常作用是从response中提取出需要的数据，或者是生成更多接下来要爬取的URL。

  这个方法跟其他的`Request`回调函数一样，需要至少返回`Request`、`dict`、`item`中的一种。

- **log(message, [, level, component])**

  通过`Spider`的`logger`输出信息。

- **closed(reason)**

  `Spider`关闭时调用此方法。

- **from_crawler(crawler, args, \*\*kwargs)*

  这是`Scrapy`用来创建你的`Spiders`的方法。

  一般来说你不需要直接的重写这个方法，因为它扮演着`__init__`方法的代理。这个方法会在新实例中设置`crawler`和`settings`，这样你才能在`Spider`中访问这两个属性。

- **`__init__()`**

  负责爬虫的初始化

- **make_requests_from_url(url)**

  该方法会被start_requests()调用，负责实现生成Request请求对象。

### 1.3 运行流程

首先，在`Scrapy`中，只要我们按照规则定义好了相应的代码，那么很多时候`Scrapy`会自动的帮我们调度运行程序。

1. 生成的模板中有一个`start_urls`参数，这是这个`Spider`的运行入口，`Scrapy`会自动的将这个参数中的url发送到`Downloader`进行下载，并且自动的调用`parse`方法来处理获得的`response`。

   如果希望自定义爬虫的开始方式，那么删除这个参数，并自定义一个`start_urls`方法即可。

2. 在`parse`方法处理`response`的过程中，我们一般会有可能获取两种对象，一个是最终我们从网页上提取的数据，这种数据我们会将其保存为`item`对象。另一种是我们获取的接下来要访问的url，这一种我们会将其生成为一个`Request`对象。

   这两种获得的对象，我们都会使用`yield`将其返回出去。`Scrapy`会自动检测对象的类型，如果是`item`，则会将其发送到`item pipeline`进行存储等处理，如果是`Request`对象，会送往调度器然后统一再次往`Downloader`发送进行访问。

3. 每一个`Request`对象，都会在生成的时候绑定一个回调函数，用来处理这个请求返回的响应结果。

通过以上这几个步骤一直循环往复，则可以将我们想要抓取的数据抓取完毕。

## 2 `CrawlSpider`类

`Scrapy`提供了一些有用的通用爬虫类，可以用来实现某些特定功能，例如`CrawlSpider`，这些通用爬虫类都是继承自`Spider`类，在此基础上添加一些功能。

`CrawlSpider`的主要用处是通过一条或者多条固定的规则（`rules`），来抓取页面上所有的连接。这常常被用来做整站爬取。

### 2.1 创建爬虫文件

与普通创建爬虫文件不同的是，需要指定模板。

首先与创建项目，然后使用`crawl`模板创建爬虫文件：

```
scrapy genspider -t crawl news_spider https://news.163.com/
```

然后打开查看生成的爬虫原始文件：

```
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class NewsSpiderSpider(CrawlSpider): # 注意这里继承的类，它的父类也是Spider类
    name = 'news_spider'
    allowed_domains = ['https://news.163.com/']
    start_urls = ['http://https://news.163.com//']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    ) # 定义网页爬取规则

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i
```

可以看到，这个模版的主体继承自`CrawlSpider`类。

### 2.2 `CrawlSpider`类

```
class scrapy.spiders.CrawlSpider
```

`CrawlSpider`是Spider的派生类，Spider类的设计原则是只爬取`start_urls`列表中的网页，而`CrawlSpider`类中定义了一些规则（rule）来提取跟进link的方便机制，从而爬取的网页中获取link并继续爬取。

这种通用爬虫类主要用来抓取常见的网站，对于一些特定的网站可能不是非常适合，但是更具有通用性。它可以通过定义规则来跟踪连接。

**该类同样继承自Spider类。**

除了那些继承自`Spider`的属性和方法，这个类支持以下的新属性和方法：

- **rules**

  这是由一个或者多个`Rule`对象组成的元祖。每一个`Rule`对象定义了一种抓取网页的行为。如果多个`Rule`匹配到了同一个连接，那么在列表中位置靠前的将会被使用。

  

  ```
  class scrapy.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
  ```

  参数：

  - `link_exractor` - 一个定义了如何从页面上提取连接的`LinkExtractor`对象。

    **`LinkExtractors`链接提取器（找到满足规则的`url`进行爬取）**

    ```
    class scrapy.linkextractors.LinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=True, unique=True, process_value=None)
    ```

    主要参数介绍：

    - `allow`:允许的`url`，所有满足这个正则表达式的`url`都会被提取
    - `deny`:禁止的`url`,所有满足这个正则表达式的`url`都不会被提取（优先级高于不提取）
    - `allow_domains`:允许的域名，在域名中的`url`才会被提取
    - `deny_domains`:禁止的域名，在域名中的`url`不会被提取
    - `restrict_xpaths`:使用`xpath`提取
    - `restrict_css`:使用`css`提取

  - `callback` - 用来处理返回的`Response`的回调函数。这个回调函数需要接收`Response`作为第一个参数，并且需要返回一个包含`Item`或者`Request`的列表

    注意：不能使用`parse()`方法作为回调函数，`CrawlSpider`是通过`parse()`方法来实现逻辑的，所以如果重写了`parse()`方法，`CrawlSpider`将无法运行。

  - `cb_kwargs` - 包含关键字参数的字典，可以传递给回调函数。

  - `follow` - 是否跟踪访问那些使用`Rule`提取出来的连接。当`callback`为None时，`follow`默认为True。除此以外，`follow`默认为False。

    也就是说，在没有指定`callback`时，`CrawlSpider`会默认继续访问从response中使用`Rule`提取的连接。如果指定了`callback`，那么response将会教给回调函数处理。

  - `process_links` - 用来处理从response中提取出的连接的函数。主要的作用是用来筛选目标。

  - `process_request` - 用来处理从response中提取出来的每一个request。需要返回一个`Request`对象，或者None（用来筛选request）。

- **parse_start_url(response)**

  这个方法是用来处理`start_urls`返回的响应。这个方法需要至少返回`Item`对象，`Request`对象，或者一个包含前两种对象的可迭代对象。

### 2.3 文件编写

```
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import News163Item


class NewsSpiderSpider(CrawlSpider):
    name = 'news_spider'
    # allowed_domains = ['https://news.163.com/']
    start_urls = ['https://news.163.com//']

    rules = (
        Rule(LinkExtractor(allow=r'.*\.163\.com/\d+/\d+/\d+/[0-9A-Z]+.*'), callback='parse_item', follow=True),
    ) # 定义提取链接的规则，系统自动匹配到url并生成请求，返回的响应传到回调函数进行处理

    def parse_item(self, response):
        item = News163Item()
        title = response.xpath('//div[@class="post_content_main"]/h1/text()').extract_first()
        item["title"] = title
        time = response.xpath('//div[@class="post_time_source"]/text()').extract_first()
        item["time"] = time
        item["url"] = response.url
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        yield item
```

## 3 `XMLFeedSpider`类

## 4 `CSVFeedSpider`类