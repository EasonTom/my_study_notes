## 1 反爬措施

通常防止爬虫被反主要有以下几个策略：

- 动态设置User-Agent（随机切换User-Agent，模拟不同用户的浏览器信息）
- 禁用Cookies（也就是不启用cookies middleware，不向Server发送cookies，有些网站通过cookie的使用发现爬虫行为）
      可以通过COOKIES_ENABLED 控制 CookiesMiddleware 开启或关闭
- 设置延迟下载（防止访问过于频繁，设置为 2秒 或更高）
  在setting.py文件中加入一行`DOWNLOAD_DELAY = 5`设置。
- Google Cache 和 Baidu Cache：如果可能的话，使用谷歌/百度等搜索引擎服务器页面缓存获取页面数据。
- 使用IP地址池：VPN和代理IP，现在大部分网站都是根据IP来ban的。

## 2 日志

### 2.1 Log levels

Scrapy提供5层logging级别:

CRITICAL - 严重错误(critical)

ERROR - 一般错误(regular errors)

WARNING - 警告信息(warning messages)

INFO - 一般信息(informational messages)

DEBUG - 调试信息(debugging messages)

### 2.2 设置

1. 终端输入

在运行爬虫时通过`-L`参数指定日志等级，默认为DEBUG。详见项目命令。

```
scrapy crawl spidername -L INFO
```

或者直接关闭日志

```
scrapy crawl spidername --nolog
```

2. setting.py文件设置

LOG_ENABLED 默认: True，启用logging

LOG_ENCODING 默认: 'utf-8'，logging使用的编码

LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名

LOG_LEVEL 默认: 'DEBUG'，log的最低级别

LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print "hello" ，其将会在Scrapy log中显示。

3. 在爬虫文件中使用日志信息

在查看Spider类的源代码时，你会发现它有这样一个方法：

```
class Spider(object_ref):    
    @property
    def logger(self):
        logger = logging.getLogger(self.name)
        return logging.LoggerAdapter(logger, {'spider': self})
```

在爬虫中可以直接通过该方法添加日志。

```
self.logger.info("开始爬取网站")
```

**通过日志信息可以方便查看错误位置**