# Scrapyd

scrapyd是运行scrapy爬虫的服务程序，由scrapy官方提供，它支持以http命令方式发布、删除、启动、停止爬虫程序。而且scrapyd可以同时管理多个爬虫，每个爬虫还可以有多个版本。

特点：
1. 可以避免爬虫源码被看到。
2. 有版本控制。
3. 可以远程启动、停止、删除


详细用法请参考[scrapyd官方文档](http://scrapyd.readthedocs.io/en/stable/overview.html)。

## 1 安装

```
sudo pip install scrapyd
```

## 2 运行

终端直接输入`scrapyd`启动服务。

## 3 配置

- 端口转发

- 设置远程连接
- 
要想能够远程连接需要修改配置文件，配置文件是python解释器下的dist-packages/scrapyd/default_scrapyd.conf文件，将`bind_address`修改为`0.0.0.0`，即所有ip都可访问。



## 4 安装 scrapyd-deploy（客户端）

主要有两种安装方式：

```
pip install scrapyd-client（安装的版本可能不是最新版本）
```

或者从 <http://github.com/scrapy/scrapyd-client> 中下源码，

运行`python setup.py install` 命令进行安装。

## 5 发布项目

1. 打开scrapy.cfg文件

```
[settings]
default = news163.settings

[deploy]
url = http://127.0.0.1:6800/
project = news163
```

url代表你要推送的scrapyd服务的地址，所以要将url地址换成服务器的ip地址。

2. 上传

```
scrapyd-deploy <target> -p <project> --version <version>
```

target   scrapy.cfg文件中deploy后的内容[deploy:target]，内容可自定义。可写可不写。

project   scrapy.cfg文件中的project名，可以自定义

version  版本号，可自定义，默认为时间戳。

## 6 scrapyd API接口

启动爬虫：

```
curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider_name
```

停止爬虫：

```
curl http://localhost:6800/cancel.json -d project=spider_name -d job=job_id
```
删除项目

```
curl http://localhost:6800/delproject.json -d project=project_name
```
列出当前项目的爬虫
```
curl http://localhost:6800/listspider.json？project=spider_name
```
列出所有项目列表
```
curl http://localhost:6800/listproject.json
```

更多API接口可查看[官网](<http://scrapyd.readthedocs.io/en/latest/api.html>)。