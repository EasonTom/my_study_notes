# Scrapy分布式部署

[官方文档](https://scrapy-redis.readthedocs.io/en/stable/)

[源码位置](https://github.com/rmax/scrapy-redis)

[博客](https://www.cnblogs.com/kylinlin/p/5198233.html)

scrapy_redis是基于scrapy框架的组件，用于开发scrapy项目的分布式和部署，其特点在于利用了redis数据库来保存队列和去重。

scrapy_redis提供的组件:Scheduler  Dupefilter  Pipeline Spider.

**特征：**

- 分布式爬取

您可以启动多个spider工程，相互之间共享单个redis的requests队列。最适合广泛的多个域名网站的内容爬取。



- 分布式数据处理

爬取到的scrapy的item数据可以推入到redis队列中，这意味着你可以根据需求启动尽可能多的处理程序来共享item的队列，进行item数据持久化处理



- Scrapy即插即用组件

Scheduler调度器 + Dupefilter过滤器，Item Pipeline，基本spider

1. 首先Slaver端从Master端拿任务（Request、url）进行数据抓取，Slaver抓取数据的同时，产生新任务的Request便提交给 Master 处理；

2. Master端只有一个Redis数据库，负责将未处理的Request去重和任务分配，将处理后的Request加入待爬队列，并且存储爬取的数据。

Scrapy-Redis默认使用的就是这种策略，我们实现起来很简单，因为任务调度等工作Scrapy-Redis都已经帮我们做好了，我们只需要继承RedisSpider、指定redis_key就行了。

缺点是，Scrapy-Redis调度的任务是Request对象，里面信息量比较大（不仅包含url，还有callback函数、headers等信息），可能导致的结果就是会降低爬虫速度、而且会占用Redis大量的存储空间，所以如果要保证效率，那么就需要一定硬件水平。


## 1  安装

```
sudo pip install scrapy_redis
```
[源码](https://github.com/rolando/scrapy-redis.git)
下载源码
```
git clone https://github.com/rolando/scrapy-redis.git
```

## 2 常用配置

1. 使用了scrapy_redis的去重组件，在redis数据库里做去重(必须)

DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"



2. 使用了scrapy_redis的调度器，在redis里分配请求(必须)

SCHEDULER = "scrapy_redis.scheduler.Scheduler"



3. 在redis中保持scrapy-redis用到的各个队列，从而True允许暂停和暂停后恢复，也就是不清理redis queues(可选)

SCHEDULER_PERSIST = True



4. 通过配置RedisPipeline将item写入key为 spider.name : items 的redis的list中，供后面的分布式处理item(可选)

这个已经由 scrapy-redis 实现，不需要我们写代码，直接使用即可

```
ITEM_PIPELINES = {

    'scrapy_redis.pipelines.RedisPipeline': 100

}
```


5. 指定redis数据库的连接参数(**必须**)

REDIS_HOST = '127.0.0.1'

REDIS_PORT = 6379

## 3 reids设置

1、  “项目名:items” 

list 类型，保存爬虫获取到的数据item

内容是 json 字符串

  

2、  “项目名:dupefilter”

set类型，用于爬虫访问的URL去重

内容是 40个字符的 url 的hash字符串



3、  “项目名: start_urls”

List 类型，用于获取spider启动时爬取的第一个url，爬虫启动后需要在其中加入一个厨师url



4、  “项目名:requests”

zset类型，用于scheduler调度处理 requests

内容是 request 对象的序列化 字符串

## 4 scrapy项目重构

如何把一个Scrapy项目改造成Scrapy-Redis增量式爬虫

前提: 安装Scrapy-Redis

- 原有的爬虫代码不用改动
- 在setting配置文件中添加如下配置
1. 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据,从而实现请求去重的持久化


    DUPEFILTER_CLASS = “scrapy_redis.dupefilter.RFPDupeFilter”


2. 增加了调度的配置, 作用: 把请求对象存储到Redis数据, 从而实现请求的持久化.


    SCHEDULER = “scrapy_redis.scheduler.Scheduler”

3. 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据


    SCHEDULER_PERSIST = True

4. redis_url配置


    REDIS_URL = ‘reds://127.0.0.1:6379/2’

5. 如果需要把数据存储到Redis数据库中, 可以配置RedisPipeline

```
ITEM_PIPELINES = { 
# 把爬虫爬取的数据存储到Redis数据库中 
‘scrapy_redis.pipelines.RedisPipeline’: 400, 
}
```

### 4.1 spider文件

```
import scrapy
from scrapy_redis.spiders import RedisSpider
from ..items import BossjobItem


class BossSpiderSpider(RedisSpider):
    name = 'boss_spider'
    redis_key = 'bossjob:start_url'

    def parse(self, response):
        pass
```

爬虫继承自scrapy_redis提供的RedisSpider或者RedisCrawlSpider，它们的区别类似于scrapy.Spider和scrapy.CrawlSpider。

`redis_key`指定初始网址，网址需要写入redis数据库，键就是`redis_key`的值，值就是需要爬取的初始网址。

### 4.2 配置文件

settings文件加入：

```
# 指定scrapy_redis的调度器
SCHEDULER = 'scrapy_redis.scheduler.Scheduler'

# 指定去重
DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'

# 保持队列
SCHEDULER_PERSIST = True

# 数据库配置
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379

# 加入redis提供的pipeline，使得到的结果保存进redis（可以自己定义）
ITEM_PIPELINES = {
  #  'bossjob.pipelines.BossjobPipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 200
}
```

