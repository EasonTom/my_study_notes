# `Scrapy`简介

[官方文档](https://docs.scrapy.org/en/latest/)

`Scrapy`是一个开源的爬虫框架，目前在Python爬虫领域基本处于一家独大的地位，只要说起Python的开源爬虫框架，那基本指的都是`Scrapy`。

在`Scrapy`的官网上，则宣称的是`Scrapy`是一个快速、简单、容易扩展的爬虫框架。`Scrapy`确实是容易扩展的，通过各种管道(`Pipeline`)和中间件(`Middleware`)，能够非常方便的扩展`Scrapy`的功能。但其实相对来说，做一些简单的、抓取量小的任务时，`Scrapy`是比较笨重的，这种任务更适合使用`Requests`去做。

而如果你抓取的数据量较大的话，使用`Scrapy`是非常合适的，因为底层是基于`Twisted`，所以`Scrapy`是天生异步的，就基本不用再去额外的考虑并发的问题了。同时它提供了对于爬虫来说非常全面的功能，而且扩展功能跟二次定制也很方便。配合类似于`Scrapy-redis`之类的库也可以很简单的实现分布式的爬虫。

## 1 安装

在Linux以及Mac系统上，安装`Scrapy`非常方便，使用pip安装即可：

```
pip install Scrapy
```

如果连接不上可以换源

```
pip install scrapy -i <http://pypi.douban.com/simple> --trusted-host <pypi.douban.com>

```

但是如果在windows系统上安装的话，会遇到一个非常麻烦的问题，`Scrapy`的依赖库`Twisted`安装不上，会导致安装失败。这个问题可以用以下办法解决。

1. 到下面这个网址去下载`Twisted`已经编译好的wheel文件安装，安装好后再使用pip安装`Scrapy`。

   <https://www.lfd.uci.edu/~gohlke/pythonlibs/>

   wheel文件的安装方法请自行百度。

## 2 Scrapy构架解析

![scrapy架构](Scrapy架构.png)



### 2.1 组件

`Scrapy`的很大一个优点就是其组织架构清晰，整体结构由一个个组件构成，每个组件负责各自的功能。

其主要的运行流程中有如下几个最主要的组件：

- **Engine**

  引擎，整个爬虫系统的数据流程处理，都由`Engine`来进行触发 ，是`Scrapy`的核心。

- **Scheduler**

  调度器，负责维护Request的队列，将`Engine`传递过来的Request放入队列之中，并在`Engine`请求时将一个Request交给`Engine`。

- **Downloader**

  下载器，根据接收到的Request，去下载相应的网页内容，并生成Response返回给`Spider`。

- **Spider**

  爬虫，这个部分常常由我们自己编写，在`Spider`中需要定义网页抓取和解析的所有流程和规则。

- **Item**

  数据，也就是在`Spider`中抓取网页并解析后，我们最终要获得的数据结果，在`Scrapy`中专门定义了这样一个数据结构用于保存抓取到的数据。格式和使用方式与字典类似。

- **Item Pipeline**

  数据管道，负责处理在`Spider`中得到的`Item`。这个`Pipeline`主要用处就是清洗数据，处理数据，存储数据。

- **Downloader Middleware**

  下载器中间件，处于`Downloader`与`Engine`之间。

- **Spider Middleware**

  爬虫中间件，处于`Spider`和`Engine`之间。

### 2.2 数据流

![img](Scrapy架构-2.png)

在`Scrapy`中，数据流基本由`Engine`控制，`Engine`就好像一颗心脏，不断地推动整个数据流的流动。以下是数据流的基本流程：

1. `Engine`首先会打开一个起始url，并找到相对应的`Spider`来处理这个url访问返回的响应结果。

2. 在`Spider`的处理过程中，会将接下来要访问的url包装成Request，`Engine`会将Request从`Spider`取出，交给`Scheduler`进行调度。

3. `Engine`从`Scheduler`获取一个Request。

4. `Engine`将获取到的Request经由**下载器中间件（Downloader Middleware）**发送给`Downloader`进行下载并生成相应的Response。

5. `Engine`从`Downloader`获取一个Response。

6. `Engine`将获取的Response经由**爬虫中间件（Spider Middleware）**发送给相对应的`Spider`，由`Spider`来对Response进行解析。

   在解析过程中，可能会产生两种产物，一种是`Item`，一种是Request。产生的Request会再次沿着步骤1的流程运行下去，而产生的`Item`则会进入下一步。

7. `Engine`从`Spider`获取一个`Item`。

8. `Engine`将获取的`Item`发送给`Item Pipeline`进行相对应的存储、清洗等处理。

   `Item Pipeline`处理的过程中，同样可能会生成新的Request，这时候生成的Request会直接放入`Scheduler`中，从步骤3再次执行下去。



