# `Scrapy`应用流程

本节通过简单的例子介绍一下`Scrapy`的使用流程。

## 1 创建项目

`Scrapy`的主要使用方式是命令行，例如，开始一个`Scrapy`爬虫的第一步是创建一个`Scrapy`项目，而这一步需要使用命令行来完成，在命令行中输入以下命令：

```
scrapy startproject projectname
```

`projectname`则是你创建的项目的名称，命令执行之后会在当前目录下生成一个如下结构的目录：

```
projectname/
    scrapy.cfg            # 部署的配置文件
    projectname/          # 项目的Python模块，导入自己代码的话需要从这里导入
        spiders/          # 一个将会存放你的爬虫的目录
            __init__.py
        __init__.py
        items.py          # 配置Item的文件，定义需要抓取的目标
        middlewares.py    # 配置中间件的文件
        pipelines.py      # 配置管道的文件，用来处理爬取后的信息
        settings.py       # 项目的配置文件
```

这个目录相当于是一个`Scrapy`给你定制好的模板，节省了我们许多的时间，我们只需要按照`Scrapy`的规则在相应的文件中编写代码，就可以很快速的完成一个爬虫项目。

## 2 Items

创建好项目后需要先定义Items。

Items对象用来保存爬取到的数据，相当于自定义容器。

首先规划好自己所要爬取的结构化信息，然后将这些信息在对应的爬虫项目中的items.py文件中进行相应地定义。

items.py文件：

```
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy  # 导入文件


class IaskItem(scrapy.Item): # 定义一个类，默认类名为'项目名'+'Item'，可以自定义类名和定义多个容器，但是必须继承自'scrapy.Item'类
    # define the fields for your item here like:
    # name = scrapy.Field()
    question = scrapy.Field() # 根据这个格式定义一个字段
    answers = scrapy.Field()
```

可以看出，定义一个结构化数据，只需要将`scrapy`下的Field类实例化即可。



接下来通过Python shell命令行来更加深入了解下Items。

```
>>> import scrapy
>>> class Person(scrapy.Item):
	name = scrapy.Field()
	age = scrapy.Field()
	job = scrapy.Field()

	
>>> eason = Person(name='eason', age=20, job='teacher')
>>> print(eason)
{'age': 20, 'job': 'teacher', 'name': 'eason'}
>>> 
```

我们首先导入了scrapy，然后定义了一个人的容器，定义了三个字段，最后通过关键字传参的方式实例化Person类，再查看实例发现：

- 数据以字典的方式存储
- 原数据项名转变为字典的字段名，数据项对应的值为字典相应字段对应的值
- 可以通过字典的取值方式取字段的值
- 可以通过赋值的方式修改某个字段的值
- .keys()方法获取对象所有字段名
- .items()方法获取对象的项目视图

### 3 爬虫

接下来就要开始编写爬虫了，同样的，`Scrapy`提供了一个命令，能让我们快速的生成一个`Spider`模板放到`spiders`目录中，进入项目目录，输入以下命令：

```
scrapy genspider spidername mydomain.com
```

`spidername`指的是你创建的`Spider`名称，`mydomain.com`则是你这个爬虫抓取目标的域名。

命令执行后会在`spiders`目录下生成一个如下的`spidername.py`文件：

```
# -*- coding: utf-8 -*-
import scrapy


class SpidernameSpider(scrapy.Spider):
    name = 'spidername'
    allowed_domains = ['mydomain.com']
    start_urls = ['http://mydomain.com/']

    def parse(self, response):
        pass
```

这就是一个标准的`Spider`模板了，还有其它模板详见下一章爬虫类的介绍。

然后编写爬虫文件，以爱问网站为例：

```
# -*- coding: utf-8 -*-
import scrapy
from ..items import IaskItem # 导入容器，项目级文件导入，'..'表示上级目录


class IaskSpiderSpider(scrapy.Spider):
    name = 'iask_spider'
    # allowed_domains = ['iask.sina.com.cn/c/74.html']
    start_urls = ['https://iask.sina.com.cn/c/74.html']

    def parse(self, response):
        question_list = response.xpath('//li[@class="list"]') 使用xpath提取问题信息，返回selector对象列表

        for question in question_list: # 遍历每个问题
            item = IaskItem() # 初始化容器
            detail_url = question.xpath('//div[@class="question-title"]/a/@href').extract_first() # 从selector中提取问题详情的url
            print(detail_url)
            detail_url = 'http://iask.sina.com.cn' + detail_url
            item["question"] = question.xpath('//div[@class="question-title"]/a/@title').extract_first() # 将问题写入Item容器中
            yield scrapy.Request(url=detail_url, callback=self.detail_parse, meta={'items': item}) # 返回详情页的Request请求，指定回调函数，并把item容器传入回调函数（此时容器中只有question）

    def detail_parse(self, response): # 定义详情页回调函数

        item = response.meta.get('items') # 取出item容器
        answers = response.xpath('//li[@t="disploy"]') # 提取答案的selector对象
        answer_list = []
        # 提取答案的内容、回答人和时间，组成一个字典
        for answer_info in answers:
            answer = answer_info.xpath('.//pre/text()').extract_first()
            respondent = answer_info.xpath('.//p[@class="user-name"]/a/text()').extract_first()
            time = answer_info.xpath('.//p[@class="time"]/text()').extract_first()
            answer = {
                'answer': answer,
                'respondent': respondent,
                'time': time
            }
            answer_list.append(answer)
		
        item['answers'] = answer_list # 将所有答案放进一个列表然后放入容器中
        return item # 返回容器

```

Spider类的详细介绍见下一章。

页面提取方法见Scrapy选择器的介绍。

## 4 PipeLines

当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。

每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：

- 验证爬取的数据(检查item包含某些字段，比如说name字段)
- 查重(并丢弃)
- 将爬取结果保存到文件或者数据库中

这里以入库为例：

```
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
from pymongo import MongoClient # 导入数据库


class IaskPipeline(object):

    def __init__(self):
    	# 连接数据库
        client = MongoClient(host="127.0.0.1", port=27017)
        conn = client["iask"]
        self.db = conn["item"]
	
	# 写入数据
    def process_item(self, item, spider):
        self.db.insert(dict(item))
        return item
```

为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置，就像下面这个例子:

```python
# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    #'mySpider.pipelines.SomePipeline': 300,
   'iask.pipelines.IaskPipeline': 300,
}
```

分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）

## 5 中间件

详见`Scrapy`中间件。

## 6 settings

settings中的配置针对整个项目，如果需要定制每个请求的配置，就需要在中间件中完成。

```
# -*- coding: utf-8 -*-

# Scrapy settings for iask project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'iask' # 项目名

SPIDER_MODULES = ['iask.spiders'] # 项目会在这里面搜索爬虫文件
NEWSPIDER_MODULE = 'iask.spiders' # 创建模板文件的路径


# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'  # 设置UA

# Obey robots.txt rules
ROBOTSTXT_OBEY = False  # 关闭robot协议

# 并发数量，默认16
# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
DOWNLOAD_DELAY = 3 # 设置延时，防止反爬
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16 # 单个网站的最大请求值
#CONCURRENT_REQUESTS_PER_IP = 16  # 单个ip的最大请求值

# Disable cookies (enabled by default)
COOKIES_ENABLED = True  # 设置cookie

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False # 控制台

# 默认请求头
# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
# 爬虫中间件
#SPIDER_MIDDLEWARES = {
#    'iask.middlewares.IaskSpiderMiddleware': 543,
#}

# Enable or disable downloader middlewares
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
# 下载中间件
#DOWNLOADER_MIDDLEWARES = {
#    'iask.middlewares.IaskDownloaderMiddleware': 543,
#}

# Enable or disable extensions
# See https://doc.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
# 配置实体管道
ITEM_PIPELINES = {
   'iask.pipelines.IaskPipeline': 300,
}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

```

## 7 运行爬虫

在写好`Spider`之后，就可以使用命令来运行这个`Spider`了：

```
scrapy crawl spidername
```

从这里开始，你的爬虫就可以欢快的开始运行了。



